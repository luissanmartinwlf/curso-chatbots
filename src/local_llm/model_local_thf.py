from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# checkpoint = "HuggingFaceTB/SmolLM-135M-Instruct"
# checkpoint = "HuggingFaceTB/SmolLM2-360M-Instruct"
# checkpoint = "microsoft/Phi-3-mini-4k-instruct"
checkpoint = "HuggingFaceTB/SmolLM2-1.7B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForCausalLM.from_pretrained(checkpoint)

messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant, you speak spanish. Answer to the questions in a short way.",
    },
    {"role": "user", "content": "Cu√°l es la mejor manera de cocinar cebollas?"},
]
input_text = tokenizer.apply_chat_template(messages, tokenize=False)


pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)
response = pipe(input_text, return_full_text=False, max_new_tokens=200)
bot = response[0]["generated_text"]
if bot.startswith("assistant\n"):
    bot = bot[len("assistant\n") :].strip()

print(bot)
